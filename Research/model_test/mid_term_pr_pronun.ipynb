{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sykgV4oIWyLA",
        "outputId": "e42c3dd1-50d8-40cb-a2d0-4ce44c41623d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper\n",
        "!pip install python-dotenv\n",
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u-XxIG1Mb_X",
        "outputId": "87c082f3-e1c7-48bf-8fd6-77924fb7fb56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.10/dist-packages (20240930)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.6)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.8.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import whisper\n",
        "import math\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import soundfile as sf\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/kakaotech/피칭/')\n",
        "load_dotenv()\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "\n",
        "# model = whisper.load_model(\"large\")\n",
        "def transcribe_audio(file_path): # STT 처리\n",
        "    global model\n",
        "    result = model.transcribe(file_path, fp16=False)\n",
        "    return result[\"text\"]\n",
        "\n",
        "def generate_tts_audio(script): # TTS\n",
        "    num = math.ceil(len(script)/4000)\n",
        "    if math.ceil(num) == 1:\n",
        "      response = client.audio.speech.create(\n",
        "        model=\"tts-1\",\n",
        "        voice=\"alloy\",\n",
        "        input = script\n",
        "      )\n",
        "      response.stream_to_file(\"TTS.mp3\")\n",
        "\n",
        "    else:\n",
        "      for i in range(num):\n",
        "        response = client.audio.speech.create(\n",
        "          model=\"tts-1\",\n",
        "          voice=\"alloy\",\n",
        "          input = script[4000*(i):4000*(i+1)]\n",
        "        )\n",
        "        response.stream_to_file(\"TTS\" + str(i) + \".mp3\")\n",
        "\n",
        "      audio1 = AudioSegment.from_mp3(\"TTS0.mp3\")\n",
        "      audio2 = AudioSegment.from_mp3(\"TTS1.mp3\")\n",
        "      combined_audio = audio1 + audio2\n",
        "      combined_audio.export(\"TTS.mp3\", format=\"mp3\")\n",
        "\n",
        "\n",
        "def compare_audio_similarity(file1, file2): # 음성 유사도 비교\n",
        "    y1, sr1 = librosa.load(file1, sr=None)\n",
        "    y2, sr2 = librosa.load(file2, sr=None)\n",
        "\n",
        "    min_len = min(len(y1), len(y2))\n",
        "    y1 = y1[:min_len]\n",
        "    y2 = y2[:min_len]\n",
        "\n",
        "    mfcc1 = librosa.feature.mfcc(y=y1, sr=sr1)\n",
        "    mfcc2 = librosa.feature.mfcc(y=y2, sr=sr2)\n",
        "\n",
        "    similarity = cosine_similarity(mfcc1.T, mfcc2.T).mean()\n",
        "    return similarity\n",
        "\n",
        "def calculate_text_accuracy(reference_text, transcribed_text): # 텍스트 유사도\n",
        "    reference_words = reference_text.split()\n",
        "    transcribed_words = transcribed_text.split()\n",
        "\n",
        "    correct_words = sum(1 for ref, trans in zip(reference_words, transcribed_words) if ref == trans)\n",
        "    accuracy = correct_words / max(len(reference_words), len(transcribed_words))\n",
        "    return accuracy\n",
        "\n",
        "def calculate_presentation_score(audio_file_path, script_text): # 최종 점수 산출 함수\n",
        "    transcribed_text = transcribe_audio(audio_file_path)\n",
        "    print(\"Transcribed Text:\", transcribed_text)\n",
        "\n",
        "    text_accuracy = calculate_text_accuracy(script_text, transcribed_text)\n",
        "    print(\"Text Accuracy:\", text_accuracy)\n",
        "\n",
        "    tts_file_path = \"TTS.mp3\"\n",
        "    generate_tts_audio(script_text)\n",
        "\n",
        "    audio_similarity = compare_audio_similarity(audio_file_path, tts_file_path)\n",
        "    print(\"Audio Similarity:\", audio_similarity)\n",
        "\n",
        "    final_score = (text_accuracy * 0.5) + (audio_similarity * 0.5)\n",
        "    return final_score"
      ],
      "metadata": {
        "id": "yjPxqotkpS_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file_path = \"attention is all you need.mp3\"  # 발표자의 음성 파일 경로\n",
        "script_text = \"꼼꼼한 딥러닝 논문 리뷰와 코드 실습. 이번 시간에 리뷰할 논문은 현대 딥러닝 기반의 자연어처리 기술의 핵심 아키텍처가 되고 있는 트랜스포머입니다. 트랜스포머 논문의 원래 제목은 Attention is all you need입니다. 논문의 제목에서 알 수 있듯이 트랜스포머라는 아키텍처에는 이 Attention이라고 하는 것이 가장 메인 아이디어로서 사용이 된다는 걸 알 수 있습니다. 실제로 트랜스포머는 Attention이라는 메커니즘을 전적으로 활용하는 아키텍처입니다. 트랜스포머가 나오게 된 계기를 이해하기 위해서 딥러닝 기반의 기계 번역 발전 과정에 대해 확인해 보겠습니다. 2021년 기준으로 최신 자연어처리 쪽 고성능 모델들은 이런 트랜스포머 아키텍처를 기반으로 하고 있습니다. 최근까지 화제가 되었던 GPT와 BERT는 모두 이러한 트랜스포머의 아키텍처를 적절히 활용하여 좋은 성능을 내고 있습니다. 대표적으로 GPT는 트랜스포머의 디코드 아키텍처를 활용했고 BERT는 트랜스포머의 인코드 아키텍처를 활용했다는 점입니다. 자연어처리 테스크 중에서 가장 대표적이면서 중요한 테스크 중 하나는 기계 번역입니다. 실제로 기계 번역 기술의 발전 과정을 확인해 보시면 1986년도 즈음에 RNN이 제한되었고 그로부터 약 10년 정도가 지난 뒤에 LSTM이 등장하였습니다. 이러한 LSTM을 활용하면 다양한 시퀀스 정보를 모델링할 수 있는데요. 대표적으로 주가 예측, 주기함수 예측 등이 가능합니다. 이러한 LSTM을 활용해서 2014년도에는 딥러닝 기반 기술로 시퀀스 정보를 모델링할 수 있는데요. 시퀀스 시퀀스가 등장하였습니다. 시퀀스 시퀀스는 현대의 딥러닝 기술들이 다시 빠르게 나오기 시작한 시점인 2014년도에 이러한 LSTM을 활용해서 고정된 크기의 컨텍스트 벡터를 사용하는 방식으로 번역을 수행하는 방법을 제안하였습니다. 다만 이러한 시퀀스 시퀀스 모델이 나왔을 때의 시점만 하더라도 고정된 크기의 컨텍스트 벡터를 쓰고 있기 때문에 소스 문장을 전부 고정된 크기의 한 벡터에다가 압축을 할 필요가 있다는 점에서 성능적인 한계입니다. 이 시스템을 통해 LSTM을 활용한 것에 대한 성능적인 한계가 존재했습니다. 이후에 어텐션 메커니즘이 제한된 논문이 나오면서 이러한 시퀀스 시퀀스 모델에 어텐션 기법을 적용하여 성능을 더 끌어올릴 수가 있었고요. 이제 그 이후에 트랜스포머 논문에서는 그냥 RNN 자체를 사용할 필요가 없다는 아이디어로 오직 어텐션 기법에 의존하는 아키텍처를 설계했더니 성능이 훨씬 좋아지는 것을 보여주었습니다. 즉 이 트랜스포머를 기점으로 해서 더 이상 다양한 자연어처리 테스크에 대해서 RNN 기반의 아키텍처를 사용하지 않고 어텐션 메커니즘을 더욱더 많이 사용하게 되었습니다. 그래서 어텐션 메커니즘이 등장한 이후로부터는 입력 시퀀스 전체에서 정보를 추출하는 방향으로 연구 방향이 발전되어 왔다고 할 수 있습니다. 물론 이후에 나온 논문들 중에서도 RNN을 활용하는 아키텍처도 많이 존재하지만 전반적인 추세 자체는 어텐션 기법을 더욱더 활용하는 이런 트랜스포머의 아키텍처를 따르는 방식으로 다양한 고성능 모델들이 제한되고 있습니다. 그렇다면 기존에 제한되었던 시퀀스트 시퀀스는요. 시퀀스 모델에는 어떤 한계점이 존재할까요? 기존 시퀀스트 시퀀스 모델의 한계점이라고 한다면 이 컨텍스트 벡터 V에 소스 문장의 정보를 압축한다는 점입니다. 이때 병목 현상이 발생할 수 있기 때문에 성능 하락의 원인이 될 수 있는데요. 현재 예시를 확인해 보시면 대표적인 시퀀스트 시퀀스 모델을 활용한 기계 번역 예시라고 할 수 있습니다. 왼쪽에 있는 독일어 문장, 즉 각각의 단어들로 구성된 하나의 시퀀스가 들어왔을 때 이렇게 중간에서 하나의 고정된 크기의 컨텍스트 벡터로 바꾼 뒤에 다시 이러한 컨텍스트 벡터로부터 출력 문장을 만들어내는 것을 확인할 수 있습니다. 즉 한쪽의 시퀀스에서부터 다른 한쪽의 시퀀스를 만든다는 의미에서 시퀀스트 시퀀스 모델이라고 부를 수 있습니다. 결과적으로 이렇게 영어 출력 문장이 나오는 걸 확인할 수 있고요. 다만 이때 이러한 시퀀스트 시퀀스 아키텍처를 확인해 보시면 매번 단어가 입력될 때마다 히든 스테이트 값을 갱신하는 걸 확인할 수 있습니다. 이런 식으로 단어가 입력될 때마다 이전까지 입력되면 단어들에 대한 정보를 포함하고 있는 히든 스테이트 값을 받아서 매번 이런 식으로 히든 스테이트 값을 새롭게 갱신합니다. 즉 이런 식으로 각각의 단어가 차례대로 순서에 맞게 입력될 때마다 히든 스테이트 값이 갱신되어 이러한 히든 스테이트 값은 이전까지 입력되었던 단어들에 대한 정보를 갖고 있기 때문에 이렇게 마지막 단어가 들어왔을 때 그때의 히든 스테이트 값은 소스 문장 전체를 대표하는 하나의 컨텍스트 벡터로서 사용할 수가 있다는 것입니다. 그렇기 때문에 이렇게 마지막 단어가 들어왔을 때의 히든 스테이트 값을 하나의 컨텍스트 벡터로서 이 컨텍스트 벡터 안에는 앞에 등장했던 소스 문장에 대한 문맥적인 정보를 담고 있다고 가정하는 것입니다. 그렇기 때문에 이러한 컨텍스트 벡터로부터 출발해서 이렇게 출력을 수행하는 디코더 파트에서는 매번 출력 단어가 들어올 때마다 이러한 컨텍스트 벡터로부터 출발해서 마찬가지로 히든 스테이트를 만들어서 매번 출력을 내보냅니다. 이렇게 그 다음 단계에서는 이렇게 출력했던 단어가 다시 입력으로 들어와서 반복적으로 이전까지 출력했던 단어에 대한 정보를 가지고 있는 히든 스테이트와 같이 입력을 받아 새롭게 히든 스테이트를 갱신하는 걸 확인할 수 있습니다. 이런 식으로 디코더 파트에서는 매번 히든 스테이트 값을 갱신하면서 이렇게 히든 스테이트 값으로부터 출력 값이 엔드 오브 시퀀스가 나올 때까지 반복합니다. 그래서 엔드 오브 시퀀스가 나왔을 때 출력 문장 생성을 마치게 되고요. 이렇게 출력된 정보인 good evening이 나오는 걸 확인할 수 있습니다. 가장 기본적인 형태의 시퀀스 시퀀스 모델의 동작 원리입니다. 다만 확인해 보시면 이렇게 소스 문장을 대표하는 하나의 컨텍스트 벡터를 만들어야 한다는 점에서 이렇게 고정된 크기의 컨텍스트 벡터의 정보를 압축하려고 하면 이러한 입력 문장은 어떨 때는 짧기도 하고 어떨 때는 길기도 하기 때문에 그러한 다양한 경우의 수에 대해서 항상 소스 문장의 정보를 고정된 크기로 가지고 있는 것은 전체 성능에서 병목 현상의 원인이 될 수 있습니다. 그래서 이러한 문제를 조금이나마 완화하기 위한 아이디어로 이 고정된 크기의 컨텍스트 벡터를 매번 이 디코더의 RNN 셀에서 참고하도록 만들어서 조금 더 성능을 개선할 수 있습니다. 이렇게 하게 되면 이 컨텍스트 벡터에 대한 정보가 이 디코더 파트의 RNN 셀을 거침에 따라서 정보가 손실되는 정도를 더 줄일 수 있기 때문에 출력되는 문장이 길어진다고 하더라도 각각의 출력되는 단어에 이러한 컨텍스트 벡터에 대한 정보를 다시 한번 넣어 줄 수 있어서 성능이 기존보다 조금 더 향상될 수 있습니다. 그래서 이러한 컨텍스트 벡터에 대한 정보를 다시 한번 넣어 줄 수 있어서 성능이 기존보다 조금 더 향상될 수 있습니다. 다만 이런 식으로 접근한다고 하더라도 여전히 이 소스 문장을 하나의 벡터에 압축해야 된다는 점은 동일하기 때문에 병목현상은 여전히 발생합니다. 즉 현재의 문제 상황이라고 한다면 하나의 문맥 벡터, 즉 컨텍스트 벡터가 소스 문장의 모든 정보를 가지고 있어야 하기 때문에 성능이 저하될 수 있다는 것입니다. 그렇다면 디코더 파트에서는 하나의 문맥 벡터에 대한 정보만 가지고 있는 게 아니라 출력 단어를 만들 때마다 매번 소스 문장에서의 출력값들 전부를 입력으로 받으면 어떨까요? 라는 아이디어가 나올 수 있는 거죠. 최신 GPU는 많은 메모리와 그리고 빠른 병렬 처리를 지원하기 때문에 소스 문장의 시퀀스 길이가 길다고 하더라도 그러한 소스 문장을 구성하는 각각의 단어에 대한 출력값들 전부를 특정 행렬에다가 기록해 놓았다가 소스 문장에 대한 전반적인 내용들을 매번 출력할 때마다 반영할 수 있기 때문에 성능이 좋아질 것을 기대할 수 있습니다. 다시 말해 하나의 고정된 크기의 컨텍스트 벡터에 담지 말고 그냥 소스 문장에서 나왔던 출력값들 전부를 매번 입력으로 받아서 일련의 처리 과정을 거쳐서 출력 단어를 만들도록 하면 성능이 더 좋아질 수 있다는 겁니다. 지금 보이는 아키텍처가 바로 시퀀스 시퀀스에 어텐션 메커니즘을 적용한 아키텍처인데요. 이렇게 어텐션 메커니즘을 적용해서 인코더 파트의 모든 출력을 참고하도록 만들 수가 있습니다. 실제로 파이톨치와 같은 프레임워크에서는 단순히 RNN이나 LSTM 같은 걸 사용하도록 만들면 이렇게 매번 전체 시퀀스 길이에 맞는 아웃풋 값들이 따로 출력 값들이 나오게 되는데요. 이제 그걸 그대로 이용해서 실제로 어텐션 메커니즘을 간단하게 구현할 수도 있습니다. 전반적인 내용을 확인해 보시면 이렇게 매번 단어가 출력돼서 히든스테이트가 나올 때마다 그냥 이 값들을 전부 다 출력 값으로써 그냥 별도의 배열에다가 다 기록해 놓습니다. 그래서 이런 식으로 각각의 단어를 거치면서 갱신되는 히든스테이트 값들을 매번 다 가지고 있는 거예요. 이렇게 해 줌으로써 이렇게 매 단어가 들어왔을 때에 히든스테이트 값을 전부 다 출력할 수 있습니다. 그리고 이 값들을 어떻게든 참고해서 이렇게 출력 단어가 매번 생성될 때마다 이러한 소스 문장 전체를 반영하겠다라는 아이디어라고 보시면 되겠습니다. 실제로는 이렇게 디코더 파트에서 매번 히든스테이트를 갱신하게 되는데 이때 현재 단계에서 히든스테이트 값을 만든다고 하면 바로 이전에 히든스테이트 값을 이용해서 이 출력 단의 히든스테이트 값과 이렇게 소스 문장 단의 히든스테이트 값을 서로 묶어서 별도의 행렬 곱을 수행해서 각각 에너지 값을 만들어 냅니다. 이때 그 에너지 값은 내가 현재 어떠한 단어를 출력하기 위해서 소스 문장에서 어떤 단어에 초점을 둘 필요가 있는지를 수치화해서 표현한 값입니다. 그래서 그러한 에너지 값에 소프트 맥스를 취해서 확률 값을\"\n",
        "score = calculate_presentation_score(audio_file_path, script_text)\n",
        "print(\"Final Presentation Score:\", score)"
      ],
      "metadata": {
        "id": "vvPEJ1NLNgR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file_path = \"도비 발표.m4a\"\n",
        "script_text = transcribe_audio(audio_file_path)"
      ],
      "metadata": {
        "id": "ai0CGkB2pdAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "script_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "collapsed": true,
        "id": "nKHBt8GcOm9k",
        "outputId": "ca3c3ebd-de28-4bdb-8578-776e01dca701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' 발표 시작하겠습니다. 안녕하세요. 저는 이번에 인공지능과 파이썬으로 금융 데이터 분석해보기 with 자연어 처리로 발표하게 될 김대현이라고 합니다. 일단 제 소개를 간단하게 드리자면, 저는 지금 모드웨어 연구소라는 곳에서 비전 랭키지 쪽으로 모드웨어 연구를 하고 있고요. 또한 카카오테크 부트캡에서 생성형 인공지능 과정을 수강하고 있습니다. 최근에 고파콘이라고 고언어 관련해서 열리는 큰 행사가 있는데, 거기에 준비위원회로 참가를 했고, 이제 또 JRC라는 로봇회사에서 로보틱스 엔지니어로 근무한 경력이 있습니다. 일단 크게 4가지 주제를 다뤄볼 거예요. TF, IDF, N-gram, Covert, DinoSync 메서드랑 Positive Index Reality, 즉 이제 신뢰지수에 대해서 한번 얘기를 해보려고 합니다. 일단은 이 프로젝트를 먼저 시작하게 된 계기가 바로 이거였어요. 일반적으로 우리가 주식을 할 때 처음에 화면을 보면 어떠한 걸 봐야 될지 모르겠는 거예요. 그래서 약간 복잡한 리서치 화면 및 뉴스 커뮤니티에 통한 투자 결정에 어려움이 존재해서 이 프로젝트를 진행하게 되었고, 또한 이제 일단은 한국 투자증권 기준으로 보면은 카테고리가 일단은 종목이 2400개가 있어요. 근데 이거를 10개의 섹터로만 구분을 해서 의사결정에 대해서 어려운 점이 있었습니다. 그래서 이 프로젝트에 진행하게 되었고, 요약해드리자면 이제 우리가 이걸 왜 하게 되었는지, 중요정보에 쉽게 접근하고, 이제 정확한 투자 도구에 대해서, 그래서 이 프로젝트에 진행하게 되었고, 요약해드리자면 이제 우리가 이걸 왜 하게 되었는지, 중요정보에 쉽게 접근하고, 이제 정확한 투자 도구에 대해서, 정확한 투자 동향을 파악하면서 이제 유망한 분야에 탐색하고, 또한 이제 여론과 어느 정도 상관관계 분석이 있는지 그걸 파악하면서 개발해보려고 개발했습니다. 일단은 개발 설계 및 연구 방법에 대해서 설명을 드리면은, 일단은 모델을 선정이랑 이제 어떻게 프로젝트를 진행할지 플래닝을 했고요. 건 다음에 이제 데이터 수집 및 전처리, VHC 세부나 키워드 감성 분석, 적용 데이터 분석, 건 다음에 이제 데이터 수집 및 전처리, VHC 세부나 키워드 감성 분석, 적용 데이터 분석, 그 데이터 시각화 이렇게 다섯 가지로 구성을 했습니다. 일단 우리가 일반적으로 인공지능 과연한 프로젝트를 시작을 할 때 이제 데이터 수집을 해야 되잖아요. 그래서 일반적으로 가져와있을 금융 데이터가 이제 수치적으로 이제 즉 CSV 형태로 가져와있을 금융 데이터는 이제 한국의 이제 한국 거래소 같은 데가 있어요. 이제 거기서 정보 코드랑 이제 과연한 주요 내용들, 상장에 이러한 내용들이 있습니다. 이제 거기서 정보 코드랑 이제 과연한 주요 내용들이 상장에 이러한 내용들이 있습니다. 이러한 내용들을 가져왔고, 그래서 판다스 라이벌을 활용해가지고 이제 결측치랑 중복값을 제거했습니다. 그리고 이제 TF IDF라는 걸 사용해가지고 이제 그 문서 내에서 과이라는 단어가 얼마 정도 나오는지 그거에 대한 빈도를 분석을 했고, 그 다음에 이제 Ngram이라는 즉 SLM 계열의 언어 모델 사용해가지고 단어나 이제 문장의 확률값을 할당을 해요. 그래서 이렇게 입력된 문장을 N기에 단위로 적용을 합니다. 그래서 이렇게 입력된 문장을 N기에 단위로 적용을 합니다. 그래서 이렇게 입력된 문장을 N기에 단위로 적용을 합니다. 그래서 이게 은행문장을 N로 분석을 해서 분석을 했습니다. 또한 이제 이 TF IDF랑 Ngram을 사용해서 이제 어느 정도 빈도 값에 대한 걸 파악을 했다면 이제 금융문장 네이터세스로 모델 성능을 평가하는 작업을 진행했어요. 또한 이제 이 TF IDF랑 Ngram을 사용해서 이제 어느 정도 빈도 값에 대한 걸 파악을 했다면 이제 금융문장 네이터세스로 모델 성능을 평가하는 작업을 진행했어요. 물론 1대 Pitorch 프레임워크를 사용했고 크게 3가지 모델을 사용해봤습니다. 이제 Cobalto랑 KEC Eletra Carla, QoElectra Pradana received in NYS. 코어 엔렉트라는 모델을 사용했었는데 이 중에 코버트 모델이 0.8870 정도의 Accuracy를 보여가지고 가장 높은 정확도를 보여서 사용했습니다 그래서 이제 코버트 모델을 어떻게 활용을 했냐 라고 간단하게 설명 드리면은 코버트 모델은 이제 버트 모델을 한국어로 프리트레이닝 모델이에요 그래서 이제 그 모델을 트랜스포머 기반으로 해서 사용을 했고 그 다음에 이제 이 모델을 가지고 감성 분석을 진행했습니다 크게 세 가지 라벨을 달았었거든요 이제 물론 지도학습 방식으로 진행을 했고 0이 포지티브 긍정 1이 내추럴 중립 2가 네거티브 이제 중립으로 해놓고 이제 진행을 했습니다 또한 이제 버트 모델의 아키텍처에 대해서 간단하게 설명 드리면은 일단은 우리가 앞에서 이제 금융 데이터셋을 가져와서 한다고 했잖아요 이제 그 문장을 단어 단위로 분리를 합니다 이제 예를 들어서 올해 주식시장 이 크게 상승했습니다 점이라는 이런 데이터셋이 있으면은 단어 단위로 저희는 분리를 했어요 이제 오늘 주식시장 이 크게 상승했습니다 점 이렇게 해가지고 토큰아이저를 사용해서 문장을 단어 단위로 분리를 했고요 그래서 이제 그 버트 모델의 인풋 임베링 레이어에 넣어가지고 진행을 했습니다 또한 이제 인코더 레이어에 들어가면 이제 리니어 이제 리니어 레이어를 거쳐가지고 이제 셀프어테이션 메커니즘이 적용이 돼요 이거에 대해서는 앞에 앞에 설명드린 자료에 나와 있듯이 이제 쿼리랑 키의 개념으로 구성이 되어 있는데 이제 이 글을 보시면 이렇게 나와있는데 이제 쿼리 키 이제 해가지고 이제 소프트맥스 함수를 변환해가지고 스코어를 계산을 해요 스코어를 기반해 가지고 이제 그 쿼리에 대해서 키가 얼마나 연관성 있는지를 분석을 합니다 또한 이제 프리트레인드 사전학습을 했을 때 이제 MLM과 NSP밖에 안 했거든요? 그 방식을 사용해 가지고 진행을 했고요 그런 다음에 이제 파인트링 같은 경우에 이제 Supervised Learning 즉 지도학습을 사용해 가지고 앞에서 설명드렸다시피 이제 이렇게 감정 부유 Labelless Label 데이터셋을 분리했습니다 또한 이제 금융 데이터셋을 어떻게 분리를 했냐 라고 이제 말씀을 드릴 수 있을 것 같은데 그게 데이터 수집, 전처리, 데이터 아이솔레이션, 데이터 분리 이렇게 세 가지 이제 분야를 나눠서 진행을 했고요 데이터 수집은 이제 DART라는 한국 기업 공무원이고요 한국 기업 공시 정보 데이터를 가져왔고 그 다음에 이제 그에 관련한 이제 Time-Storage 데이터라고 이제 시계열 데이터도 있어요 그걸 가져와서 이제 데이터 프리프로세싱 데이터 전처리를 해 가지고 이제 여기서 이제 Dynosync 메소드라는 걸 사용했습니다 그래서 이제 여기서 이제 Information은 즉 필요 있는 정보 Noise는 필요 없는 정보를 이렇게 해 가지고 분리를 했습니다 그럼 여기서 이제 Dynosync 메소드를 왜 사용했냐 라고 말씀드리면 크게 두 가지 사유가 있어요 일단은 첫 번째로 무의미한 가격 정보를 제거하면서 주식 폭등, 폭락 같은 쓸데없는 정보들을 제거하기 위해 사용을 했고 그리고 의미 없는 정보를 이제 제거했습니다 즉 분석과 무관한 뉴스를 사용을 했고 이걸 작업을 하면서 이제 정확하고 의미 있는 정보를 알기 위해 사용했습니다 그럼 이제 데이터 표현을 어떻게 했냐 일단은 일반적으로 우리가 저희가 서비스화를 할 거면 일단 프론트엔드를 만들어야 되잖아요? 네 그걸 이제 React랑 이제 JS를 써 가지고 개발을 했고 데이터 시각화 하는 거는 이제 D3랑 Chart.js 라이브러리가 있어요 시각화를 해주는 그 라이브러리 사용을 했고 그래프 교환하는 거는 이제 즉 Node Node Express 사용해 가지고 데이터 서버를 저장해서 불러오는 방식으로 구현했습니다 이렇게 앞에 있는 그림 보시듯이 이렇게 디자인을 했고요 이것은 시연 영상입니다 그래서 어떤 결론을 낼 수 있었냐 라고 하면 크게 네 가지가 있어요 월별 양지수를 판단하는 거랑 재무지표 ESG 타 업종과의 비교 키워드와 금융 정보에 대해서 파악할 수 있었습니다 그리고 긍정지수 신뢰도라는 게 있는데 이거 어떤 거냐면은 이제 저희가 실제 만들 예측한 거랑 이제 실제 ETF 수익률이 관계가 있는지를 나타내려고 만든 건데요 이거는 즉 실제 주가의 노이즈 제거해서 이제 긍정지수와 수익률의 상공간지를 분석을 합니다 이제 그리고 이제 시장의 긍정 반응과 부정 반응이 더 높은 더 낮은 수익률 연결된다는 것과 이제 긍정지수가 급격히 상승하라 하락할 때 수익률이 따라오는 경향을 이제 볼 수 있었습니다 그래서 제가 내릴 수 있는 결론과 의논점에 대해서 얘기해 주자면은 일단은 저희 플랫폼을 통해서 성장 가능성과 평가 정보를 제공한다는 것 두 번째 이제 최신 트렌드 및 시장 동향을 분석할 수 있다는 것 그리고 세 번째는 데이터 수집 제한 시각화에 미흡할 수 있다는 점을 보였습니다 이거는 과연한 레퍼런스이고요 제가 수집을 해가지고 사용했기 때문에 확인해 주시면 될 것 같습니다 이상으로 발표 마치겠습니다'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_tts_audio(script_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyJl3EH5RSbV",
        "outputId": "81c9d6c2-694c-4480-b8d3-83245663aea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-d4b89a8e9357>:41: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
            "  response.stream_to_file(\"TTS\" + str(i) + \".mp3\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = calculate_presentation_score(audio_file_path, \"TTS.mp3\") #\n",
        "print(\"Final Presentation Score:\", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcEEC4rRRXLR",
        "outputId": "4d513d75-9424-4b34-bacb-04117385e7ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribed Text:  발표 시작하겠습니다. 안녕하세요. 저는 이번에 인공지능과 파이썬으로 금융 데이터 분석해보기 with 자연어 처리로 발표하게 될 김대현이라고 합니다. 일단 제 소개를 간단하게 드리자면, 저는 지금 모드웨어 연구소라는 곳에서 비전 랭키지 쪽으로 모드웨어 연구를 하고 있고요. 또한 카카오테크 부트캡에서 생성형 인공지능 과정을 수강하고 있습니다. 최근에 고파콘이라고 고언어 관련해서 열리는 큰 행사가 있는데, 거기에 준비위원회로 참가를 했고, 이제 또 JRC라는 로봇회사에서 로보틱스 엔지니어로 근무한 경력이 있습니다. 일단 크게 4가지 주제를 다뤄볼 거예요. TF, IDF, N-gram, Covert, DinoSync 메서드랑 Positive Index Reality, 즉 이제 신뢰지수에 대해서 한번 얘기를 해보려고 합니다. 일단은 이 프로젝트를 먼저 시작하게 된 계기가 바로 이거였어요. 일반적으로 우리가 주식을 할 때 처음에 화면을 보면 어떠한 걸 봐야 될지 모르겠는 거예요. 그래서 약간 복잡한 리서치 화면 및 뉴스 커뮤니티에 통한 투자 결정에 어려움이 존재해서 이 프로젝트를 진행하게 되었고, 또한 이제 일단은 한국 투자증권 기준으로 보면은 카테고리가 일단은 종목이 2400개가 있어요. 근데 이거를 10개의 섹터로만 구분을 해서 의사결정에 대해서 어려운 점이 있었습니다. 그래서 이 프로젝트에 진행하게 되었고, 요약해드리자면 이제 우리가 이걸 왜 하게 되었는지, 중요정보에 쉽게 접근하고, 이제 정확한 투자 도구에 대해서, 그래서 이 프로젝트에 진행하게 되었고, 요약해드리자면 이제 우리가 이걸 왜 하게 되었는지, 중요정보에 쉽게 접근하고, 이제 정확한 투자 도구에 대해서, 정확한 투자 동향을 파악하면서 이제 유망한 분야에 탐색하고, 또한 이제 여론과 어느 정도 상관관계 분석이 있는지 그걸 파악하면서 개발해보려고 개발했습니다. 일단은 개발 설계 및 연구 방법에 대해서 설명을 드리면은, 일단은 모델을 선정이랑 이제 어떻게 프로젝트를 진행할지 플래닝을 했고요. 건 다음에 이제 데이터 수집 및 전처리, VHC 세부나 키워드 감성 분석, 적용 데이터 분석, 건 다음에 이제 데이터 수집 및 전처리, VHC 세부나 키워드 감성 분석, 적용 데이터 분석, 그 데이터 시각화 이렇게 다섯 가지로 구성을 했습니다. 일단 우리가 일반적으로 인공지능 과연한 프로젝트를 시작을 할 때 이제 데이터 수집을 해야 되잖아요. 그래서 일반적으로 가져와있을 금융 데이터가 이제 수치적으로 이제 즉 CSV 형태로 가져와있을 금융 데이터는 이제 한국의 이제 한국 거래소 같은 데가 있어요. 이제 거기서 정보 코드랑 이제 과연한 주요 내용들, 상장에 이러한 내용들이 있습니다. 이제 거기서 정보 코드랑 이제 과연한 주요 내용들이 상장에 이러한 내용들이 있습니다. 그래서 이런 내용들을 가져왔고, 그래서 판다스 라이브러리를 활용해가지고 이제 결측치랑 중복값을 제거했습니다. 그리고 이제 TF-IDF라는 걸 사용해가지고 이제 그 문서 내에서 그 과이라는 단어가 얼마 정도 나오는지 그거에다 빈도를 분석을 했고, 그런 다음에 이제 N-gram이라는 즉 SLM 계열의 언어 모델 사용해가지고 단어나 이제 문장의 확률 값을 할당을 해요. 이제 그래서 이렇게 입력된 문장은 N계 단위로 되고요. 그래서 이렇게 입력된 문장은 N계 단위로 되고요. 이렇게 입력된 문장은 N계 단위로 되고요. 또한 이제 이 TF-IDF라던지 N-gram을 사용해서 이제 어느 정도 빈도 값에 대한 것을 파악했다면 이제 금융분장 데이터세스로 모델별 성능을 평가하는 작업을 진행했어요. 또한 이제 이 TF-IDF라던지 N-gram을 사용해서 이제 어느 정도 빈도 값에 대한 것을 파악했다면 이제 금융분장 데이터세스로 모델별 성능을 평가하는 작업을 진행했어요. 물론 이때 파이토치 프레임워크를 사용을 했고, 크게 세 가지 모델을 사용했습니다. 이제 코발트랑 KEC-ELECTRA, 코어-ELECTRA라는 모델을 사용했었는데. 이 중에 코버트 모델이 0.8870 정도의 Accuracy를 보여서 가장 높은 정확도를 보여서 사용했습니다 그래서 이제 코버트 모델을 어떻게 활용을 했냐고 간단하게 설명 드리면 코버트 모델은 버트 모델을 한국어로 프리트레이 하는 모델이에요 그래서 이제 그 모델을 트랜스포머 기반으로 사용을 했고 그 다음에 이제 이 모델을 가지고 감성 분석을 진행했습니다 크게 세 가지 라벨을 달았었거든요 이제 물론 지도학습 방식으로 진행을 했고 0이 포지티브 긍정, 1이 리트럴 중립, 2가 네거티브 중립으로 해놓고 진행을 했습니다 또한 이제 볼트 모델의 아키텍처에 대해서 간단하게 설명 드리면 일단은 우리가 앞에서 금융 데이터셋을 가져와서 한다고 했잖아요 그 문장을 단어 단위로 분리를 합니다 예를 들어서 올해 주식시장이 크게 상승했습니다. 이라는 데이터셋이 있으면 단어 단위로 저희는 분리를 했어요 이제 오늘 주식시장이 크게 상승했습니다 점 이렇게 해가지고 토큰아이저를 사용해서 문장을 단어 단위로 분리를 했고요 그래서 이제 그 볼트 모델의 인풋 인베링 레이어에 넣어가지고 진행을 했습니다 또한 이제 인코더 레이어에 들어가면 이제 리니어 레이어를 거쳐가지고 이제 셀프 어텐션 메커니즘이 적용이 돼요 이거에 대해서는... 음... 앞에... 앞에 설명드렸지만... 음... 앞에... 앞에 설명드렸지만... 앞에 설명드린 자료에 나와 있듯이 이제 쿼리랑 키의 개념으로 구성이 되어 있는데 그러니까 이제... 이 글에 보시면 이렇게 나와 있는데 이제 쿼리 키 이제 해가지고 이제 소프트맥스 함수를 변환해가지고 스코어를 계산을 해요 스코어를 기반해가지고 이제 그 쿼리에 대해서 키가 얼마나 연관성 있는지를 분석을 합니다 또한 이제 프리트레인드 사전 학습을 했을 때 이제 MLM과 NSP 방식을 사용해가지고 진행을 했고요 그런 다음에 이제 파인트레인 같은 경우에 이제 슈퍼바이오 파인트레인 같은 경우에 이제 슈퍼바이오 즉 지도학습을 사용해가지고 앞에서 설명드렸다시피 이제 이렇게 감정 부유 라벨 데이터셋을 분리했습니다 또한 이제 금융 데이터셋을 어떻게 분리를 했냐라고 이제 말씀을 드릴 수 있을 것 같은데 그게 데이터 수집, 전처리, 데이터 아이솔레이션, 데이터 분리 이렇게 세 가지 이제 분야를 나눠서 진행을 했고요 데이터 수집은 이제 DART라는 한국기업 공시정보 데이터를 가져왔고 그런 다음에 이제 그에 관련한 이제 타임시티가 있는 데이터가 있고 타임시티 데이터라고 이제 시계의 데이터도 있어요 그걸 가져와서 이제 데이터 프리프로세싱, 데이터 전처리를 해가지고 이제 여기서 이제 Dynosync 메소드라는 걸 사용했습니다 그래서 이제 여기서 이제 information은 즉 필요 있는 정보, noise는 필요 없는 정보를 이렇게 해가지고 분리를 했습니다 그럼 여기서 이제 Dynosync 메소드를 왜 사용했냐라고 말씀드리면 크게 두 가지 사유가 있어요 일단은 첫 번째로 무의미한 가격 정보를 제거하면서 주식 포켓을 사용하고 폭등, 폭락 같은 여기 쓸데없는 정보들을 제거하기 위해 사용을 했고 그리고 의미 없는 정보를 이제 제거했습니다 즉 분석과 무관한 뉴스를 사용을 했고 이걸 작업을 하면서 이제 정확하고 의미 있는 정보를 알기 위해 사용했습니다 그럼 이제 데이터 표현은 어떻게 했냐 일단은 일반적으로 우리가 저희가 서비스화를 할 거면 일단 프론트엔드를 만들어야 되잖아요 그걸 이제 리액트랑 이제 JS를 써가지고 개발을 했고 데이터 시각화하는 거는 이제 D3랑 차트제이셜 라이버리가 있어요 시각화를 해주는 그 라이벌을 사용을 했고 그래프 교환하는 거는 이제 즉 노드 노드 익스프레스를 사용해가지고 데이터 서버를 저장해서 불러오는 방식으로 구현했습니다 이렇게 앞에 있는 그림 보시듯이 이렇게 디자인을 했고요 이것은 시연 영상입니다 그래서 어떠한 결론을 낼 수 있었냐라고 하면은요 화면 크게 네 가지가 있어요 월별 양지수를 판단하는 거랑 재무지표 ESG 타 업종과의 비교 키워드와 금융 정보에 대해서 파악할 수 있었습니다 그리고 긍정지수 신뢰도라는 게 있는데 이거 어떤 거냐면은 이제 저희가 실제 만들 예측한 거랑 이제 실제 ETF 수익률이 관계가 있는지를 나타내려고 만든 건데요 이거는 즉 실제 주가의 노이즈 제거해서 이제 긍정지수와 수익률의 상관관계를 분석을 합니다 이제 그리고 이제 시장의 긍정반응과 부정반응이 더 높은지에 대해서는 이제 분석을 합니다. 그래서 이제 이 세 가지가 이제 더 큰 더 낮은 수익률 연결된다는 것과 이제 긍정지수가 급격히 상승하라 하락할 때 수익률이 따라오는 경향을 이제 볼 수 있었습니다 그래서 저희가 내릴 수 있는 결론과 의논점에 대해서 얘기해 주자면은 일단은 저희 플랫폼을 통해서 성장 가능성과 평가 정보를 제공한다는 것 두 번째 이제 최신 트렌드 및 시장 동향을 분석할 수 있다는 것 세 번째는 데이터 수집 제한 시각화에 미흡할 수 있다는 점을 보였습니다. 이건 과연한 레퍼런스이고요 제가 수집을 해가지고 사용했기 때문에 확인해 주시면 될 것 같습니다 이상으로 발표 마치겠습니다\n",
            "Text Accuracy: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-eb0b6e709711>:32: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
            "  response.stream_to_file(\"TTS.mp3\")\n",
            "<ipython-input-19-eb0b6e709711>:50: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y1, sr1 = librosa.load(file1, sr=None)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio Similarity: 0.8988997\n",
            "Final Presentation Score: 0.4494498372077942\n"
          ]
        }
      ]
    }
  ]
}