{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install olefile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua7HyWk8TXQN",
        "outputId": "d6378686-5684-4a38-cd27-c92b20efe9cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting olefile\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/114.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m112.6/114.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: olefile\n",
            "Successfully installed olefile-0.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44jpdmWXT67N",
        "outputId": "67bfdb32-f12a-4c25-9b26-8823d9a01d47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m194.6/244.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNrv50ZTUUTA",
        "outputId": "d5cea546-cfcb-462f-ab14-3606ef6bbdeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/232.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gethwp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB-AbNlyX_TO",
        "outputId": "4ba972a0-7a66-47ae-bcc8-5ab2a42437ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gethwp\n",
            "  Downloading gethwp-1.1.1-py3-none-any.whl.metadata (896 bytes)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.10/dist-packages (from gethwp) (0.47)\n",
            "Downloading gethwp-1.1.1-py3-none-any.whl (3.9 kB)\n",
            "Installing collected packages: gethwp\n",
            "Successfully installed gethwp-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6jk3_oTKUfG",
        "outputId": "b055bdba-e23d-4f83-c33c-c2270490f38c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "발표 시작하겠습니다. 안녕하세요. 저는 이번에 인공지능과 파이썬으로 금융 데이터 분석해보기\n",
            "with 자연어처리로 발표하게 될 김대현이라고 합니다. 일단 제 소개를 간단하게 드리자면, 저는\n",
            "지금 모듈 연구소라는 곳에서 비전 랭귀지 쪽으로 모델 연구를 하고 있고요. 또한 카카오테크\n",
            "부트캠프에서 생선형 인공지능 과정을 소강하고 있습니다. 최근에 고파콘이라고 고언어 관련해서\n",
            "열리는 큰 행사가 있는데 거기 준비위원회로 참가를 했고 이제 또 JRC 라는 로봇 회사에서\n",
            "로보틱스 엔지니어로 근무한 경력이 있습니다.\n",
            "\n",
            "일단은 크게 4 가지 주제를 다뤄볼 거에요. 이제 TF-IDF, N-GRAM, COVERT, 다이노싱 메소드,\n",
            "포지티브 인덱스 리얼리티, 즉 이제 신뢰지수에 대해서 한번 얘기를 해보려고 합니다. 일단은 이\n",
            "프로젝트를 먼저 시작하게 된 계기가 바로 이거였어요. 일반적으로 우리가 주식을 할 때 처음에\n",
            "화면을 보면 어떠한 걸 봐야 될지 모르겠는가 에요. 그래서 복잡한 리서치 화면 및 뉴스 커뮤니티에\n",
            "통한 투자 결정에 어려움이 존재해서 이 프로젝트를 진행하게 되었고 또한 이제 일단은 한국\n",
            "투자증권 기준으로 보면 카테고리가 일단은 종목이 2400 개가 있어요. 이것을 10 개의 섹터로만\n",
            "구분해서 의사결정에 대해서 어려운 점이 있었습니다. 그래서 이 프로젝트를 진행하게 되었고\n",
            "요약해드리자면 이제 우리가 이걸 왜 하게 되었는지 중요 정보에 쉽게 접근하고 이제 정확한 투자\n",
            "동향을 파악하면서 이제 유망한 분야에 탐색하고 또한 이제 여론과 어느정도 상관관계 분석이\n",
            "있는지 그거를 파악하면서 개발해 보려고 했습니다.\n",
            "\n",
            "일단은 개발 설계 및 연구방법에 대해서 설명을 드리면 일단은 모델을 선정이랑 이제 어떻게\n",
            "프로젝트를 진행할지 기획을 했고요. 건담의 이제 데이터 수집 및 전처리, 부위 체계 세부나\n",
            "키워드 감성 분석, 적용 데이터 분석, 데이터 시각화 이렇게 다섯가지로 구성을 했습니다. 일단\n",
            "일반적으로 인공지능 관련한 프로젝트를 시작할 때 이제 데이터 수집을 해야 되잖아요. 그래서\n",
            "일반적으로 가져올 수 있는 금융 데이터가 이제 수치적으로, 즉 csv 형태로 가져올 수 있는 금융\n",
            "데이터는 이제 한국의 이제 한국 거래소 같은 데가 있어요. 이제 거기서 주가 정보 코드들이랑\n",
            "관련된 주요 내용들, 상장액 이러한 내용들을 가져왔고 그래서 판다스 라이브러리을 활용해\n",
            "가지고 이제 결측치랑 중복값을 제거했습니다.\n",
            "\n",
            "\f그리고 이제 TF-IDF 라는 것을 사용해 가지고 이제 그 문서 내에서 과일 한 단어가 얼마 정도\n",
            "나오는지 그거 에다가 빈도를 분석을 했고 그 다음에 이제 N-gram 이라는 즉 slm 기열의\n",
            "언어모델을 사용해 가지고 단어나 이제 문장의 확률값을 할당을 해요 이제 그래서 이렇게 입력된\n",
            "문장을 n 개 단위로 잘라서 분석을 했습니다. 또한 이제 이 TF-IDF 랑 N-gram 을 사용해서 이제\n",
            "어느정도 빈도값에 대한 것을 파악을 했다면 이제 금융 문장 데이터셋으로 모델 별 성능을\n",
            "평가하는 작업을 진행했어요.\n",
            "\n",
            "물론 이때 파이토치 프레임워크를 사용했고 크게 세가지 모델을 사용했습니다. 이제 kobert 랑 Kc electra, ko-electra 라는 모델을 사용했었는데 이 중에 kobert 모델이 0.8870 정도의 아큐리시를\n",
            "보여 가지고 가장 높은 정확도를 보여서 사용했습니다. 그래서 이제 kobert 모델을 어떻게 활용을\n",
            "했냐라고 간단하게 설명 드리면은 kobert 모델은 이제 BERT 모델을 한국어로 Pre-trained 한\n",
            "모델이에요. 그래서 이제 그 모델을 트랜스포머 기반으로 해서 사용을 했고 그 다음에 이제 이\n",
            "모델을 가지고 감성 분석을 진행했습니다.\n",
            "\n",
            "크게 3 가지 라벨을 달았었거든요. 이제 물론 지도학습 방식으로 진행을 했고 0 이 Positive, 긍정,\n",
            "1 이 Netural, 중립, 2 가 Negative, 부정으로 해놓고 이제 진행을 했습니다. 또한 이제 BERT 모델\n",
            "아키텍처에 대해서 간단하게 설명 드리면은 일단은 우리가 앞에서 이제 금융 데이터셋을 가져와서\n",
            "한다고 했잖아요. 이제 그 문장을 단어 단위로 분리를 합니다.\n",
            "\n",
            "예를 들어서 올해 주식시장이 크게 상승했습니다. 이라는 데이터셋이 있으면은 단어 단위로\n",
            "분리를 했어요. 오늘 주식시장이 크게 상승했습니다. 이렇게 해서 Tokenizer 를 사용해서 문장을\n",
            "단어 단위로 분리를 했고요. 그래서 이제 그 BERT 모델에 Input 으로 Embedding Layer 를\n",
            "넣어가지고 진행을 했습니다. 또한 이제 인코더 레이어에 들어가면 이제 리니어 레이어를\n",
            "거쳐가지고 이제 Selt-Attention 메커니즘이 적용이 돼요. 이거에 대해서는 앞에 설명드린 자료에\n",
            "나와있듯이 이제 Query 랑 Key 의 개념으로 구성이 되어있는데 이제 이 글을 보시면 이렇게\n",
            "나와있는데 이제 Query-key 를 Softmax 를 함수를 변환해가지고 Score 를 계산을 해요. Score 에\n",
            "기반해가지고 이제 그 Query 에 대해서 Key 가 얼마나 연관성이 있는지를 분석을 합니다. 또한\n",
            "이제 Pre-trained 사전학습을 했을 때 이제 mlm 과 nsp 방식을 사용해가지고 진행을 했고요. 그\n",
            "\n",
            "\f다음에 이제 Fine-tuning 같은 경우에 이제 Superivsed Learning 즉 지도학습을 사용해가지고\n",
            "앞에서 설명 드렸다시피 이제 이렇게 감정부위를 분리했습니다. 또한 이제 금융 데이터셋을\n",
            "어떻게 분리를 했냐라고 이제 말씀을 드릴 수 있을 것 같은데 그게 데이터 수집, 전처리, 데이터\n",
            "Isolation, 데이터 분리 이렇게 세가지 이제 분야를 나눠서 진행을 했고요. 데이터 수집은 이제\n",
            "DART 라는 한국 기업 공시정보 데이터를 가져왔고 그 다음에 이제 그에 관련한 타임 시리즈\n",
            "데이터라고 이제 시계열 Preprocesing 데이터도 있어요. 그걸 가져와서 이제 데이터 열, 데이터\n",
            "전처리를 해가지고 이제 여기서 이제 Denoising 메소드라는 걸 사용했습니다.\n",
            "\n",
            "그래서 이제 여기서 이제 인포메이션은 즉 필요있는 정보, 노이즈는 필요없는 정보를 이렇게\n",
            "해가지고 분리를 했습니다. 그럼 여기서 이제 Denoising 메소드를 왜 사용했냐라고 말씀드리면은\n",
            "크게 두 가지 사유가 있었어요. 일단은 첫 번째로 무의미한 가격 정보를 제거하면서 주식 폭등,\n",
            "폭락 같은, 폭락 같은 여기 쓸데없는 정보들을 제거하기 위해서 사용을 했고 그리고 의미 없는\n",
            "정보를 이제 제거했습니다. 즉 분석과 무관한 뉴스를 사용을 했고 이거 작업을 하면서 이제\n",
            "정확하고 의미 있는 정보를 알기 위해서 사용했습니다.\n",
            "\n",
            "그럼 이제 데이터 표현을 어떻게 했냐. 일단은 일반적으로 우리가 서비스화를 할 거면 일단\n",
            "프론트엔드를 만들어야 되잖아요. 그걸 이제 React 랑 JS 를 써가지고 개발을 했고, 데이터\n",
            "시각화하는 거는 이제 D3 랑 ChartJS 라이벌이 있어요. 시각화를 해주는 그 라이벌을 사용을 했고,\n",
            "이제 그래픽을 하는 거는 이제 즉 노드, 노드 익스프레스 사용해가지고 데이터 서버를 제작해서\n",
            "불러는 방식으로 구현했습니다. 이렇게 앞에 있는 그림 보시듯이 이렇게 디자인을 했고요. 이것은\n",
            "시연 영상입니다.\n",
            "\n",
            "그래서 어떠한 결론을 낼 수 있었냐라고 하면 크게 네 가지가 있어요. 월변 양지수로 판단하는 거랑\n",
            "재무지표 ESG, 타업종과의 비교, 키워드와 금융정보에 대해서 파악할 수 있었습니다. 그리고\n",
            "긍정지수 신뢰도라는 게 있는데, 이거 어떤 거냐면은 이제 저희가 실제 만든 예측한 거랑 이제 실제\n",
            "ETF 수익률이 관계가 있는지를 나타내려고 만든 건데요. 이거는 즉 실제 주가의 노이즈를 제거해서\n",
            "이제 긍정지수와 수익률의 상공관계를 분석을 합니다. 그리고 이제 시장의 긍정 반응과 부정\n",
            "\n",
            "\f반응이 더 높은, 더 낮은 수익률 연결된다는 것과 긍정지수가 급격히 상승으로 하락할 때 수익률이\n",
            "따라오는 경향을 이제 볼 수 있었습니다.\n",
            "\n",
            "그래서 제가 내릴 수 있는 결론과 논점에 대해서 얘기해 주자면은 일단은 저희 플랫폼을 통해서\n",
            "성장 가능성과 평가 정보를 제공한다는 것, 두번째 이제 최신 트렌드 및 시장 동향을 분석할 수\n",
            "있다는 것, 그리고 세번째는 데이터 수집 제한 시각화에 미흡할 수 있다는 점을 보였습니다. 이거는\n",
            "관련한 레퍼런스 이고요. 제가 수집을 해서 사용했기 때문에 확인해 주시면 될 것 같습니다.\n",
            "이상으로 발표 마치겠습니다.\n",
            "\n",
            "\f\n"
          ]
        }
      ],
      "source": [
        "# pip install olefile\n",
        "\n",
        "import olefile\n",
        "import zlib\n",
        "import struct\n",
        "import re\n",
        "from docx import Document\n",
        "from PyPDF2 import PdfReader\n",
        "import gethwp\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_extracted_text(raw_text):\n",
        "    \"\"\"\n",
        "    Extracted HWPX 텍스트를 정제하는 함수.\n",
        "\n",
        "    단계별로 불필요한 패턴과 문자를 제거하고, 공백과 문장 부호를 정리합니다.\n",
        "\n",
        "    Args:\n",
        "        raw_text (str): 정제할 원본 텍스트.\n",
        "\n",
        "    Returns:\n",
        "        str: 정제된 텍스트.\n",
        "    \"\"\"\n",
        "\n",
        "    # 0. '^숫자'로 시작하는 모든 라인 제거\n",
        "    # 예: '^1.\\n^2.\\n^3)\\n' 등과 같은 패턴을 제거\n",
        "    text = re.sub(r'^\\^\\(?\\d+[.\\)]*\\n?', '', raw_text, flags=re.MULTILINE)\n",
        "\n",
        "    # 1. '7 8 ' 패턴 제거 (남아있을 경우)\n",
        "    text = re.sub(r'\\b7\\s*8\\b\\s*', '', text)\n",
        "\n",
        "    # 2. 'IAA'와 같은 특정 단어 제거 (대소문자 구분 없이)\n",
        "    text = re.sub(r'\\bIAA\\b', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # 3. Base64와 유사한 인코딩된 데이터 제거 (10자 이상)\n",
        "    # 숫자만으로 이루어진 긴 문자열은 제거하지 않도록 패턴 수정\n",
        "    text = re.sub(r'\\b(?=.*[A-Za-z])[A-Za-z0-9+/=]{10,}\\b', '', text)\n",
        "\n",
        "    # 4. 불필요한 기호 패턴 제거\n",
        "    # 예: '^1.', '^2)', '(^5)', '(^6)' 등 제거\n",
        "    text = re.sub(r'\\^\\d+[.)]', '', text)\n",
        "    text = re.sub(r'\\(\\^\\d+\\)', '', text)\n",
        "\n",
        "    # 5. 허용된 문자 외의 문자 제거\n",
        "    # 한글, 영문, 숫자, 공백, '.', ',', '!', '?', '-' 제외한 모든 문자 제거\n",
        "    text = re.sub(r'[^\\uAC00-\\uD7A3a-zA-Z0-9\\s.,!?-]', ' ', text)\n",
        "\n",
        "    # 6. 여러 공백을 단일 공백으로 축소\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # 7. 문장 부호 앞뒤의 공백 제거\n",
        "    # 예: '안녕하세요 . ' -> '안녕하세요.'\n",
        "    text = re.sub(r'\\s*([.,!?])\\s*', r'\\1', text)\n",
        "\n",
        "    # 8. 문장 부호 뒤에 단일 공백 추가 (문장 끝은 제외)\n",
        "    # 예: '안녕하세요.저는' -> '안녕하세요. 저는'\n",
        "    text = re.sub(r'([.,!?])(?!$)', r'\\1 ', text)\n",
        "\n",
        "    # 9. 앞뒤 공백 제거\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def get_hwp_text(filename):\n",
        "    f = olefile.OleFileIO(filename)\n",
        "    dirs = f.listdir()\n",
        "\n",
        "    if [\"FileHeader\"] not in dirs or [\"\\x05HwpSummaryInformation\"] not in dirs:\n",
        "        raise Exception(\"Not Valid HWP.\")\n",
        "\n",
        "    header = f.openstream(\"FileHeader\")\n",
        "    header_data = header.read()\n",
        "    is_compressed = (header_data[36] & 1) == 1\n",
        "\n",
        "    nums = []\n",
        "    for d in dirs:\n",
        "        if d[0] == \"BodyText\":\n",
        "            nums.append(int(d[1][len(\"Section\"):]))\n",
        "    sections = [\"BodyText/Section\" + str(x) for x in sorted(nums)]\n",
        "\n",
        "    text = \"\"\n",
        "    for section in sections:\n",
        "        bodytext = f.openstream(section)\n",
        "        data = bodytext.read()\n",
        "        try:\n",
        "            unpacked_data = zlib.decompress(data, -15) if is_compressed else data\n",
        "        except zlib.error as e:\n",
        "            print(f\"Decompression error: {e}\")\n",
        "            continue\n",
        "\n",
        "        section_text = \"\"\n",
        "        i = 0\n",
        "        size = len(unpacked_data)\n",
        "        while i < size:\n",
        "            header = struct.unpack_from(\"<I\", unpacked_data, i)[0]\n",
        "            rec_type = header & 0x3ff\n",
        "            rec_len = (header >> 20) & 0xfff\n",
        "\n",
        "            if rec_type == 67:\n",
        "                rec_data = unpacked_data[i + 4:i + 4 + rec_len]\n",
        "                try:\n",
        "                    decoded_text = rec_data.decode('utf-16-le', errors='ignore')\n",
        "\n",
        "                    # 한글, 숫자, 영어, 공백만 유지\n",
        "                    cleaned_text = re.sub(r'[^\\uAC00-\\uD7A3a-zA-Z0-9\\s]', '', decoded_text)\n",
        "                    section_text += cleaned_text + \"\\n\"\n",
        "                except UnicodeDecodeError as e:\n",
        "                    print(f\"Decoding error at position {i}: {e}\")\n",
        "\n",
        "            i += 4 + rec_len\n",
        "\n",
        "        text += section_text\n",
        "        text += \"\\n\"\n",
        "\n",
        "    return text\n",
        "\n",
        "def get_hwpx_text(filename):\n",
        "    \"\"\"\n",
        "    HWPX 파일에서 텍스트를 추출하고 정제하는 함수.\n",
        "\n",
        "    Args:\n",
        "        filename (str): HWPX 파일 경로.\n",
        "\n",
        "    Returns:\n",
        "        str: 정제된 텍스트 또는 None (추출 실패 시).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        hwp = gethwp.read_hwpx(filename)\n",
        "        cleaned_text = clean_extracted_text(hwp)\n",
        "        return cleaned_text\n",
        "    except Exception as e:\n",
        "        print(f\"HWPX 파일 읽기 오류: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_docx_text(filename):\n",
        "    try:\n",
        "        doc = Document(filename)\n",
        "        text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(\"Error reading DOCX file:\", e)\n",
        "        return None\n",
        "\n",
        "def get_txt_text(filename):\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(\"Error reading TXT file:\", e)\n",
        "        return None\n",
        "\n",
        "def get_pdf_text(filename):\n",
        "    \"\"\"\n",
        "    PDF 파일에서 텍스트를 추출합니다.\n",
        "\n",
        "    Args:\n",
        "        filename (str): PDF 파일 경로.\n",
        "\n",
        "    Returns:\n",
        "        str: 추출된 텍스트 또는 None (추출 실패 시).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        reader = PdfReader(filename)\n",
        "        text = \"\"\n",
        "        for page_num, page in enumerate(reader.pages):\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(\"Error reading PDF file:\", e)\n",
        "        return None\n",
        "\n",
        "def extract_text(file_path):\n",
        "    extension = file_path.split('.')[-1].lower()\n",
        "    if extension == 'hwp':\n",
        "        try:\n",
        "            return get_hwp_text(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing HWP file: {e}\")\n",
        "            return None\n",
        "    elif extension == 'docx':\n",
        "        return get_docx_text(file_path)\n",
        "    elif extension == 'txt':\n",
        "        return get_txt_text(file_path)\n",
        "    elif extension == 'pdf':\n",
        "        return get_pdf_text(file_path)\n",
        "    elif extension == 'hwpx':\n",
        "        return get_hwpx_text(file_path)\n",
        "    else:\n",
        "        print(\"Unsupported file type.\")\n",
        "        return None\n",
        "\n",
        "file_path = '/content/테스트 대본1.txt'  # hwp. docx, txt, pdf, hwpx\n",
        "text = extract_text(file_path)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_extracted_text(raw_text):\n",
        "    \"\"\"\n",
        "    Extracted HWPX 텍스트를 정제하는 함수.\n",
        "\n",
        "    단계별로 불필요한 패턴과 문자를 제거하고, 공백과 문장 부호를 정리합니다.\n",
        "\n",
        "    Args:\n",
        "        raw_text (str): 정제할 원본 텍스트.\n",
        "\n",
        "    Returns:\n",
        "        str: 정제된 텍스트.\n",
        "    \"\"\"\n",
        "\n",
        "    # 0. '^숫자'로 시작하는 모든 라인 제거\n",
        "    # 예: '^1.\\n^2.\\n^3)\\n' 등과 같은 패턴을 제거\n",
        "    text = re.sub(r'^\\^\\(?\\d+[.\\)]*\\n?', '', raw_text, flags=re.MULTILINE)\n",
        "\n",
        "    # 1. '7 8 ' 패턴 제거 (남아있을 경우)\n",
        "    text = re.sub(r'\\b7\\s*8\\b\\s*', '', text)\n",
        "\n",
        "    # 2. 'IAA'와 같은 특정 단어 제거 (대소문자 구분 없이)\n",
        "    text = re.sub(r'\\bIAA\\b', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # 3. Base64와 유사한 인코딩된 데이터 제거 (10자 이상)\n",
        "    # 숫자만으로 이루어진 긴 문자열은 제거하지 않도록 패턴 수정\n",
        "    text = re.sub(r'\\b(?=.*[A-Za-z])[A-Za-z0-9+/=]{10,}\\b', '', text)\n",
        "\n",
        "    # 4. 불필요한 기호 패턴 제거\n",
        "    # 예: '^1.', '^2)', '(^5)', '(^6)' 등 제거\n",
        "    text = re.sub(r'\\^\\d+[.)]', '', text)\n",
        "    text = re.sub(r'\\(\\^\\d+\\)', '', text)\n",
        "\n",
        "    # 5. 허용된 문자 외의 문자 제거\n",
        "    # 한글, 영문, 숫자, 공백, '.', ',', '!', '?', '-' 제외한 모든 문자 제거\n",
        "    text = re.sub(r'[^\\uAC00-\\uD7A3a-zA-Z0-9\\s.,!?-]', ' ', text)\n",
        "\n",
        "    # 6. 여러 공백을 단일 공백으로 축소\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # 7. 문장 부호 앞뒤의 공백 제거\n",
        "    # 예: '안녕하세요 . ' -> '안녕하세요.'\n",
        "    text = re.sub(r'\\s*([.,!?])\\s*', r'\\1', text)\n",
        "\n",
        "    # 8. 문장 부호 뒤에 단일 공백 추가 (문장 끝은 제외)\n",
        "    # 예: '안녕하세요.저는' -> '안녕하세요. 저는'\n",
        "    text = re.sub(r'([.,!?])(?!$)', r'\\1 ', text)\n",
        "\n",
        "    # 9. 앞뒤 공백 제거\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "qmIOAGJ3htvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gethwp\n",
        "\n",
        "try:\n",
        "    hwp = gethwp.read_hwpx('/content/테스트 대본1.hwpx')\n",
        "    cleaned_text = clean_extracted_text(hwp)\n",
        "    print(cleaned_text)\n",
        "except Exception as e:\n",
        "    print(f\"Error reading HWPX file: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibOyp_lJYD53",
        "outputId": "f5ed248c-2423-40eb-a6ca-82e4d4a3487f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "발표 시작하겠습니다. 안녕하세요. 저는 이번에 인공지능과 파이썬으로 금융 데이터 분석해보기 with 자연어처리로 발표하게 될 김대현이라고 합니다. 일단 제 소개를 간단하게 드리자면, 저는 지금 모듈 연구소라는 곳에서 비전 랭귀지 쪽으로 모델 연구를 하고 있고요. 또한 카카오테크 부트캠프에서 생선형 인공지능 과정을 소강하고 있습니다. 최근에 고파콘이라고 고언어 관련해서 열리는 큰 행사가 있는데 거기 준비위원회로 참가를 했고 이제 또 JRC 라는 로봇 회사에서 로보틱스 엔지니어로 근무한 경력이 있습니다. 일단은 크게 4 가지 주제를 다뤄볼 거에요. 이제 TF-IDF, N-GRAM, COVERT, 다이노싱 메소드, 포지티브 인덱스 리얼리티, 즉 이제 신뢰지수에 대해서 한번 얘기를 해보려고 합니다. 일단은 이 프로젝트를 먼저 시작하게 된 계기가 바로 이거였어요. 일반적으로 우리가 주식을 할 때 처음에 화면을 보면 어떠한 걸 봐야 될지 모르겠는가 에요. 그래서 복잡한 리서치 화면 및 뉴스 커뮤니티에 통한 투자 결정에 어려움이 존재해서 이 프로젝트를 진행하게 되었고 또한 이제 일단은 한국 투자증권 기준으로 보면 카테고리가 일단은 종목이 2400 개가 있어요. 이것을 10 개의 섹터로만 구분해서 의사결정에 대해서 어려운 점이 있었습니다. 그래서 이 프로젝트를 진행하게 되었고 요약해드리자면 이제 우리가 이걸 왜 하게 되었는지 중요 정보에 쉽게 접근하고 이제 정확한 투자 동향을 파악하면서 이제 유망한 분야에 탐색하고 또한 이제 여론과 어느정도 상관관계 분석이 있는지 그거를 파악하면서 개발해 보려고 했습니다. 일단은 개발 설계 및 연구방법에 대해서 설명을 드리면 일단은 모델을 선정이랑 이제 어떻게 프로젝트를 진행할지 기획을 했고요. 건담의 이제 데이터 수집 및 전처리, 부위 체계 세부나 키워드 감성 분석, 적용 데이터 분석, 데이터 시각화 이렇게 다섯가지로 구성을 했습니다. 일단 일반적으로 인공지능 관련한 프로젝트를 시작할 때 이제 데이터 수집을 해야 되잖아요. 그래서 일반적으로 가져올 수 있는 금융 데이터가 이제 수치적으로, 즉 csv 형태로 가져올 수 있는 금융 데이터는 이제 한국의 이제 한국 거래소 같은 데가 있어요. 이제 거기서 주가 정보 코드들이랑 관련된 주요 내용들, 상장액 이러한 내용들을 가져왔고 그래서 판다스 라이브러리을 활용해 가지고 이제 결측치랑 중복값을 제거했습니다. 그리고 이제 TF-IDF 라는 것을 사용해 가지고 이제 그 문서 내에서 과일 한 단어가 얼마 정도 나오는지 그거 에다가 빈도를 분석을 했고 그 다음에 이제 N-gram 이라는 즉 slm 기열의 언어모델을 사용해 가지고 단어나 이제 문장의 확률값을 할당을 해요 이제 그래서 이렇게 입력된 문장을 n 개 단위로 잘라서 분석을 했습니다. 또한 이제 이 TF-IDF 랑 N-gram 을 사용해서 이제 어느정도 빈도값에 대한 것을 파악을 했다면 이제 금융 문장 데이터셋으로 모델 별 성능을 평가하는 작업을 진행했어요. 물론 이때 파이토치 프레임워크를 사용했고 크게 세가지 모델을 사용했습니다. 이제 kobert랑 Kc -electra, ko-electra 라는 모델을 사용했었는데 이 중에 kobert 모델이 0. 8870 정도의 아큐리시를 보여 가지고 가장 높은 정확도를 보여서 사용했습니다. 그래서 이제 kobert모델을 어떻게 활용을 했냐라고 간단하게 설명 드리면은 kobert모델은 이제 BERT모델을 한국어로 Pre-trained한 모델이에요. 그래서 이제 그 모델을 트랜스포머 기반으로 해서 사용을 했고 그 다음에 이제 이 모델을 가지고 감성 분석을 진행했습니다. 크게 3 가지 라벨을 달았었거든요. 이제 물론 지도학습 방식으로 진행을 했고 0 이 Positive, 긍정, 1 이 Netural, 중립, 2 가 Negative, 부정으로 해놓고 이제 진행을 했습니다. 또한 이제 BERT모델 아키텍처에 대해서 간단하게 설명 드리면은 일단은 우리가 앞에서 이제 금융 데이터셋을 가져와서 한다고 했잖아요. 이제 그 문장을 단어 단위로 분리를 합니다. 예를 들어서 올해 주식시장이 크게 상승했습니다. 이라는 데이터셋이 있으면은 단어 단위로 분리를 했어요. 오늘 주식시장이 크게 상승했습니다. 이렇게 해서 Tokenizer를 사용해서 문장을 단어 단위로 분리를 했고요. 그래서 이제 그BERT모델에 Input으로 Embedding Layer를 넣어가지고 진행을 했습니다. 또한 이제 인코더 레이어에 들어가면 이제 리니어 레이어를 거쳐가지고 이제 Selt-Attention 메커니즘이 적용이 돼요. 이거에 대해서는 앞에 설명드린 자료에 나와있듯이 이제 Query랑 Key의 개념으로 구성이 되어있는데 이제 이 글을 보시면 이렇게 나와있는데 이제 Query-key를Softmax 를 함수를 변환해가지고 Score를 계산을 해요. Score에 기반해가지고 이제 그 Query에 대해서 Key가 얼마나 연관성이 있는지를 분석을 합니다. 또한 이제 Pre-trained 사전학습을 했을 때 이제 mlm 과 nsp 방식을 사용해가지고 진행을 했고요. 그 다음에 이제 Fine-tuning 같은 경우에 이제 Learning 즉 지도학습을 사용해가지고 앞에서 설명 드렸다시피 이제 이렇게 감정부위를 분리했습니다. 또한 이제 금융 데이터셋을 어떻게 분리를 했냐라고 이제 말씀을 드릴 수 있을 것 같은데 그게 데이터 수집, 전처리, 데이터 Isolation, 데이터 분리 이렇게 세가지 이제 분야를 나눠서 진행을 했고요. 데이터 수집은 이제 DART 라는 한국 기업 공시정보 데이터를 가져왔고 그 다음에 이제 그에 관련한 타임 시리즈 데이터라고 이제 시계열Preprocesing 데이터도 있어요. 그걸 가져와서 이제 데이터 열, 데이터 전처리를 해가지고 이제 여기서 이제 Denoising 메소드라는 걸 사용했습니다. 그래서 이제 여기서 이제 인포메이션은 즉 필요있는 정보, 노이즈는 필요없는 정보를 이렇게 해가지고 분리를 했습니다. 그럼 여기서 이제 Denoising메소드를 왜 사용했냐라고 말씀드리면은 크게 두 가지 사유가 있었어요. 일단은 첫 번째로 무의미한 가격 정보를 제거하면서 주식 폭등, 폭락 같은, 폭락 같은 여기 쓸데없는 정보들을 제거하기 위해서 사용을 했고 그리고 의미 없는 정보를 이제 제거했습니다. 즉 분석과 무관한 뉴스를 사용을 했고 이거 작업을 하면서 이제 정확하고 의미 있는 정보를 알기 위해서 사용했습니다. 그럼 이제 데이터 표현을 어떻게 했냐. 일단은 일반적으로 우리가 서비스화를 할 거면 일단 프론트엔드를 만들어야 되잖아요. 그걸 이제 React 랑 JS 를 써가지고 개발을 했고, 데이터 시각화하는 거는 이제 D3 랑 ChartJS 라이벌이 있어요. 시각화를 해주는 그 라이벌을 사용을 했고, 이제 그래픽을 하는 거는 이제 즉 노드, 노드 익스프레스 사용해가지고 데이터 서버를 제작해서 불러는 방식으로 구현했습니다. 이렇게 앞에 있는 그림 보시듯이 이렇게 디자인을 했고요. 이것은 시연 영상입니다. 그래서 어떠한 결론을 낼 수 있었냐라고 하면 크게 네 가지가 있어요. 월변 양지수로 판단하는 거랑 재무지표 ESG, 타업종과의 비교, 키워드와 금융정보에 대해서 파악할 수 있었습니다. 그리고 긍정지수 신뢰도라는 게 있는데, 이거 어떤 거냐면은 이제 저희가 실제 만든 예측한 거랑 이제 실제 ETF 수익률이 관계가 있는지를 나타내려고 만든 건데요. 이거는 즉 실제 주가의 노이즈를 제거해서 이제 긍정지수와 수익률의 상공관계를 분석을 합니다. 그리고 이제 시장의 긍정 반응과 부정 반응이 더 높은, 더 낮은 수익률 연결된다는 것과 긍정지수가 급격히 상승으로 하락할 때 수익률이 따라오는 경향을 이제 볼 수 있었습니다. 그래서 제가 내릴 수 있는 결론과 논점에 대해서 얘기해 주자면은 일단은 저희 플랫폼을 통해서 성장 가능성과 평가 정보를 제공한다는 것, 두번째 이제 최신 트렌드 및 시장 동향을 분석할 수 있다는 것, 그리고 세번째는 데이터 수집 제한 시각화에 미흡할 수 있다는 점을 보였습니다. 이거는 관련한 레퍼런스 이고요. 제가 수집을 해서 사용했기 때문에 확인해 주시면 될 것 같습니다. 이상으로 발표 마치겠습니다.\n"
          ]
        }
      ]
    }
  ]
}